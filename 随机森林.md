#1.	ID3、C4.5和CART的区别：使用场景有哪些不同

##1.1 ID3、C4.5区别
 1)用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
 2)无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只试用于小数据。当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差。

##1.2 C4.5和CART区别
CART和C45基本上是类似的算法，主要区别：
1）CART是二叉树，而不是多叉树。 
2）CART既可以用于分类也可以用于回归，而C4.5只能处理分类问题。


#2.	CART：最优二值切分点是什么，如何确定
##2.1分类树
对于给定的样本集合D,其基尼指数为: 
![Alt text](./1466734430740.png)
如果样本集合D根据特征A是否取某一可能值a被分割成D1和 D2两部分,则在特征A的条件下,集合D的GINI指数定义为:
![Alt text](./1466734497697.png)
基尼指数Gini(D)表示集合D的不确定性,基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性.**基尼指数越大,样本集的不确定性越大,所以要选取使Gini(D,A)最小的A=a分割点进行划分.**

##2.2回归树
1）选择最优切分变量j与切分点s，求解
f = ![Alt text](./1466737008222.png)
遍历变量j，对固定的切分变量j扫描切分点s，选择使上式最小值的对(j, s)。**其中$R_m$是被划分的输入空间，$c_m$是空间$R_m$对应的固定输出值(为该分类结果的平均值)**。
2）用选定的对(j, s)划分区域并决定相应的输出值：
![Alt text](./1466737170538.png) 
**最优切分点通过计算f的最小值得到。算法对子区域迭代1、2步继续。**


#3.	剪枝：
##3.1二进位编码是什么（基于MDL的决策树剪枝）？
**决策树中的最小描述长度准则**
我们应该寻求这样一种合理且较小的树，使得训练样本的大多数数据符合这棵树，把样本中不符合的数据作为例外编码，使得下面两项最小：
1）编码决策树所需的比特，它代表了猜想；
2）编码例外实例所需要的比特。

|属性|Outlook|Temperature|Humidity|Windy|类|
|-|
|1|Overcast|Hot|High|Not|N|
|2|Sunny|Mild|Normal|Very|P|
|...|...|...|...|...|...|...|
|8|Rain|Hot|High|Mediun|P|
|...|...|...|...|...|...|...|
|24|Rain|Mild|High|Very|N|
|24条记录|3个属性|3个属性|2个属性|3个属性|2个类|

![Alt text](./1466753190177.png)

对上图编码，采用深度优先遍历编码，可以表示为：
1 Outlook 0 P 1 Humidity 0 N 0 P 1 Windy 0 N 0 N 1 Temp 0 N 0 N 1 P

用1表示下一个节点的内节点，然后记下节点的相应属性；用0表示下一个节点的叶节点，并用N或P记下节点的属性。表示根节点上的Outlook需要两个比特（有四种属性）；表示下一层的Humidity需要$log_23$个比特（Outlook已被选出），表示Windy需要1个比特；表示Temp需要1个比特。这样总共要25.585比特

![Alt text](./1466753800265.png)

上图表示为：
1 Outlook 0 P 1 Humidity 0 N 0 P 0 N
上图需要13.585比特，还有一个例外编码 ，我们就是指定它在54 (3 * 3 * 2 * 3)种可能性中的位置，需要$log_254$=5.75个比特。因此总共需要19.335比特。

最小二进位编码指：“树+例外”编码描述长度（total description length , TDL）最小。树生成后，进行自下而上的剪枝，直到TDL最小。

##3.2如何剪枝？剪枝原则？预剪枝和后剪枝？
###预先剪枝
在树的生成过程中根据一定的准则（如树已达到的某高度，节点中最大的样本的比例达到的阈值）来决定是否继续扩张树。
###后剪枝
待决策树完成生成后再进行剪枝。

###Cost-Complexity Pruning（代价-复杂度剪枝法）
代价（cost）：主要指样本错分率；
复杂度（complexity）: 主要指树t的叶节点数。
树t的代价复杂度为：
$$cc（t）= E/N + a * Leaf_t $$
其中，N是决策树训练样本个数，E是决策树错分样本数
$leaf_t$是t子树的叶子树
a是用于衡量代价和复杂度之间的关系

####对于t来说，剪掉它的子树s,以t中最优叶节点代替，得到新树new_t。new_t可能会比t对于训练数据分错M个，但是new_t包含的叶节点数，却比t少($leaf_s$ - 1)个，替换之后代价复杂度为：
$$cc（t）= cc(new_t)  $$
$$=>  E/N + a * Leaf_t = （E+M）/N +　a * [leaf_t - (leaf_s - 1)]$$
$$=> a = M / (N(leaf_s - 1))$$

###CCP剪枝步骤：
一、第一步
1）计算完全决策树T_max的每个非叶节点的α值；
2）循环剪掉具有最小α值的子树，直到剩下根节点
3）得到一系列剪枝(嵌套)树{T_0,T_1,T_2,…T_m}，其中T_0就是完全决策树T_max。T_i+1是对T_i进行剪枝得到的结果
二、第二步
1）使用独立的剪枝集(非训练集)对第一步中的T_i进行评估，获取最佳剪枝树
2）标准错误SE(standart error)，公式：
![Alt text](./1466756653263.png)
其中，N'是剪枝集的大小，定义$E_i$是树$T_i$对剪枝集的错分数，
$$E' = min_i \{E_i\}$$

最佳剪枝树：T_best是满足以下条件并且包含的节点数最少的那颗剪枝树。
$$条件：E_i <= E' + SE(E')$$

其中，M是用叶节点替换t的s子树以后，增加的错分样本数。
$leaf_s$是子树s的叶节点数。

###经验风险和置信风险
$$cc（t）= E/N + a * Leaf_t $$
**经验风险：上式的代价越小，经验风险越小，表示与样本越契合。**
**置信风险：上式的复杂度越小，树越简单，树的泛化能力就越强，越通用，越好解释**

#4.	随机森林：
##4.1泛化误差
对于给定的分类器{$h_1(x), ..., h_N(x)$},定义样本点（x,y）,的**间隔函数**（Margin Function)为：
$$mg(x,y) = aV_kI(h(x) = y) - max_{(j != y)} aV_kT(h_k(x) == j)$$
其中x为输入向量，y为输出，$I(*)aV_k$为对N个分类器求平均。
mg(x,y)衡量了分类器集合将样本分对的平均票数与将其分为其他类的平均票数之差，**mg(x,y) > 0表明这个样本被该组分类器分类正确，否则被分错，mg(x,g)越大，表示分类器集合对这个样本的分类性能越好**。

分类器集合的**泛化误差定义**为：
$$PE^* = P_{X, Y} (mg(x, y) < 0)$$


##4.2随机森林和其他分类算法的对比：使用场景等

###分类算法:
逻辑回归
随机森林
SVM支持向量机

###优劣
####Logistic回归分析的优点：

1.适合需要得到一个分类概率的场景

2.实现效率较高

3.对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决；

4.逻辑回归广泛的应用于工业问题上



####逻辑回归的缺点：

1.当特征空间很大时，逻辑回归的性能不是很好；

2.不能很好地处理大量多类特征或变量；

4.对于非线性特征，需要进行转换；

5.依赖于全部的数据特征，当特征有缺失的时候表现效果不好

####逻辑回归的优点：

1.直观的决策规则

2.可以处理非线性特征

3.考虑了变量之间的相互作用

####决策树的缺点：

1.没有将排名分数作为直接结果

####SVM的优点：

1.能够处理大型特征空间

2.能够处理非线性特征之间的相互作用

3.无需依赖整个数据

####SVM的缺点：

1.当观测样本很多时，效率并不是很高

2.有时候很难找到一个合适的核函数

##实际场景分类算法使用流程

1）首当其冲应该选择的就是逻辑回归，如果它的效果不怎么样，那么可以将它的结果作为基准来参考；
2)  然后试试随机森林是否可以大幅度提升模型性能。即使你并没有把它当做最终模型，你也可以使用随机森林来移除噪声变量；
3) 如果特征的数量和观测样本特别多，那么当资源和时间充足时，使用SVM不失为一种选择。


http://www.jianshu.com/p/95e5faa3f709
